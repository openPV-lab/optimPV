{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings,os\n",
    "# remove warnings from ax\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "import ax\n",
    "from optimpv import *\n",
    "from optimpv.axBOtorch.axUtils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, copy, os\n",
    "from itertools import combinations\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "from botorch.models import SaasFullyBayesianSingleTaskGP, SingleTaskGP\n",
    "# import logging\n",
    "\n",
    "# from ray import train, tune\n",
    "# from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "# from ray.tune.search.ax import AxSearch\n",
    "\n",
    "\n",
    "\n",
    "init_notebook_plotting()\n",
    "\n",
    "# Suppress FutureWarning messages\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# ##part of the message is also okay\n",
    "warnings.filterwarnings('ignore') \n",
    "params = []\n",
    "\n",
    "mun = FitParam(name = 'l2.mu_n', value = 8e-4, bounds = [1e-5,1e-2], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = False, stepsize = None, display_name=r'$\\mu_n$', unit='m$^2$ V$^{-1}$s$^{-1}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(mun)\n",
    "\n",
    "mup = FitParam(name = 'l2.mu_p', value = 5e-5, bounds = [1e-5,1e-2], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = False, stepsize = None, display_name=r'$\\mu_p$', unit=r'm$^2$ V$^{-1}$s$^{-1}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(mup)\n",
    "\n",
    "bulk_tr = FitParam(name = 'l2.N_t_bulk', value = 1e20, bounds = [1e19,1e21], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = False, stepsize = None, display_name=r'$N_{T}$', unit=r'm$^{-3}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(bulk_tr)\n",
    "\n",
    "int_trap = FitParam(name = 'l1.N_t_int', value = 4e12, bounds = [1e11,1e13], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = False, stepsize = None, display_name=r'$N_{T,int}^{ETL}$', unit='m$^{-2}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(int_trap)\n",
    "\n",
    "Nions = FitParam(name = 'l2.N_ions', value = 1e22, bounds = [1e20,5e22], type='range', values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = False, stepsize = None, display_name=r'$C_{ions}$', unit='m$^{-3}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(Nions)\n",
    "\n",
    "R_series = FitParam(name = 'R_series', value = 1e-4, bounds = [1e-5,1e-3], type='range', values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = False, stepsize = None, display_name=r'$R_{series}$', unit=r'$\\Omega$ m$^2$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(R_series)\n",
    "\n",
    "# original values\n",
    "params_orig = copy.deepcopy(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pySIMsalabim as sim\n",
    "from pySIMsalabim.experiments.JV_steady_state import *\n",
    "\n",
    "session_path = os.path.join('/home/lecorre/Desktop/pySIMsalabim/', 'SIMsalabim','SimSS')\n",
    "simss_device_parameters = os.path.join(session_path, 'simulation_setup.txt')\n",
    "\n",
    "# Set the JV parameters\n",
    "Gfracs = [0.1,0.5,1.0] # Fractions of the generation rate to simulate\n",
    "# Gfracs = None\n",
    "UUID = str(uuid.uuid4())\n",
    "\n",
    "cmd_pars = []\n",
    "for param in params:\n",
    "    if param.name != 'l2.C_np_bulk' and param.name != 'offset_l2_l1.E_c' and param.name != 'offset_l2_l3.E_v' and param.name != 'Egap_l1.E_v' and param.name != 'offset_W_L.E_c' and param.name != 'l2.N_ions':\n",
    "        cmd_pars.append({'par':param.name, 'val':str(param.value)})\n",
    "    elif param.name == 'offset_l2_l1.E_c':\n",
    "        cmd_pars.append({'par':'l1.E_c', 'val':str(3.9-param.value)})\n",
    "        vv = 3.9-param.value\n",
    "    elif param.name == 'l2.N_ions':\n",
    "        cmd_pars.append({'par':'l2.N_cation', 'val':str(param.value)})\n",
    "        cmd_pars.append({'par':'l2.N_anion', 'val':str(param.value)})\n",
    "    elif param.name == 'l2.C_np_bulk':\n",
    "        cmd_pars.append({'par':'l2.C_n_bulk', 'val':str(param.value)})\n",
    "        cmd_pars.append({'par':'l2.C_p_bulk', 'val':str(param.value)})\n",
    "\n",
    "    elif param.name == 'offset_l2_l3.E_v':\n",
    "        cmd_pars.append({'par':'l3.E_v', 'val':str(5.53-param.value)})\n",
    "    \n",
    "    elif param.name == 'Egap_l1.E_v':\n",
    "        cmd_pars.append({'par':'l1.E_v', 'val': str(vv+param.value)})\n",
    "    \n",
    "    elif param.name == 'offset_W_L.E_c':\n",
    "        cmd_pars.append({'par':'W_L', 'val':str(vv-param.value)})\n",
    "\n",
    "\n",
    "# Run the JV simulation\n",
    "ret, mess = run_SS_JV(simss_device_parameters, session_path, JV_file_name = 'JV.dat', varFile= 'Var.dat',G_fracs = Gfracs, parallel = True, max_jobs = 3, UUID=UUID, cmd_pars=cmd_pars)\n",
    "\n",
    "# import random noise\n",
    "from numpy.random import default_rng\n",
    "# save data for fitting\n",
    "X,y = [],[]\n",
    "if Gfracs is None:\n",
    "    data = pd.read_csv(os.path.join(session_path, 'JV_'+UUID+'.dat'), sep=r'\\s+') # Load the data\n",
    "    Vext = np.asarray(data['Vext'].values)\n",
    "    Jext = np.asarray(data['Jext'].values)\n",
    "    G = np.ones_like(Vext)\n",
    "    rng = default_rng()#\n",
    "    noise = rng.standard_normal(Jext.shape) * 0.01 * Jext\n",
    "    Jext = Jext + noise\n",
    "    X= Vext\n",
    "    y = Jext\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(X,y)\n",
    "    plt.show()\n",
    "else:\n",
    "    for Gfrac in Gfracs:\n",
    "        data = pd.read_csv(os.path.join(session_path, 'JV_Gfrac_'+str(Gfrac)+'_'+UUID+'.dat'), sep=r'\\s+') # Load the data\n",
    "        Vext = np.asarray(data['Vext'].values)\n",
    "        Jext = np.asarray(data['Jext'].values)\n",
    "        G = np.ones_like(Vext)*Gfrac\n",
    "        rng = default_rng()#\n",
    "        noise = rng.standard_normal(Jext.shape) * 0.005 * Jext\n",
    "        Jext = Jext + noise\n",
    "\n",
    "        if len(X) == 0:\n",
    "            X = np.vstack((Vext,G)).T\n",
    "            y = Jext\n",
    "        else:\n",
    "            X = np.vstack((X,np.vstack((Vext,G)).T))\n",
    "            y = np.hstack((y,Jext))\n",
    "\n",
    "    # remove all the current where Jext is positive i.e. above Voc\n",
    "    X = X[y<50]\n",
    "    y = y[y<50]\n",
    "\n",
    "    plt.figure()\n",
    "    for Gfrac in Gfracs:\n",
    "        plt.plot(X[X[:,1]==Gfrac,0],y[X[:,1]==Gfrac],label='Gfrac = '+str(Gfrac))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimpv.DDfits.JVAgent import JVAgent\n",
    "metric = 'mse'\n",
    "# metric = 'mse'\n",
    "# loss = 'log10'\n",
    "# loss = 'linear'\n",
    "loss = 'soft_l1'\n",
    "\n",
    "jv = JVAgent(params, X, y, session_path, simss_device_parameters,parallel = True, max_jobs = 3, metric = metric, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimpv.axBOtorch.axBOtorchOptimizer import axBOtorchOptimizer\n",
    "from botorch.acquisition.logei import (\n",
    "    qLogExpectedImprovement,\n",
    "    qLogNoisyExpectedImprovement,\n",
    ")\n",
    "from  botorch.acquisition.monte_carlo import qUpperConfidenceBound\n",
    "from ax.modelbridge.transforms.standardize_y import StandardizeY\n",
    "from ax.modelbridge.transforms.unit_x import UnitX\n",
    "from ax.modelbridge.transforms.remove_fixed import RemoveFixed\n",
    "from ax.modelbridge.transforms.log import Log\n",
    "from ax.models.torch.botorch_modular.surrogate import Surrogate\n",
    "#import single task GP\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "# SASS = SaasFullyBayesianSingleTaskGP\n",
    "from botorch.models.fully_bayesian import SaasFullyBayesianSingleTaskGP\n",
    "\n",
    "# surr = Surrogate(SingleTaskGP)\n",
    "# model_kwargs_list = [{}]\n",
    "# for i in range(3):\n",
    "#     model_kwargs_list.append({'torch_device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),'torch_dtype': torch.double,'botorch_acqf_class':qLogNoisyExpectedImprovement,'transforms':[RemoveFixed, Log,UnitX, StandardizeY],'acquisition_options':{'tau_relu':1e-7}})\n",
    "# model_kwargs_list = [{},{'torch_device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),'torch_dtype': torch.double,'botorch_acqf_class':qLogNoisyExpectedImprovement,'transforms':[RemoveFixed, Log,UnitX, StandardizeY],},{'torch_device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),'torch_dtype': torch.double,'botorch_acqf_class':qUpperConfidenceBound,'transforms':[RemoveFixed, Log,UnitX, StandardizeY]},{'torch_device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),'torch_dtype': torch.double,'botorch_acqf_class':qLogNoisyExpectedImprovement,'transforms':[RemoveFixed, Log,UnitX, StandardizeY]}]\n",
    "# model_gen_kwargs_list = [None,{'n':2,'joint_optimization':True},{'n':2,'joint_optimization':True},{'n':2,'joint_optimization':True}]\n",
    "model_gen_kwargs_list = None\n",
    "# parameter_constraints = [f'l2.mu_p - l2.mu_n <= {0}']\n",
    "parameter_constraints = None\n",
    "# print(len(model_kwargs_list))\n",
    "from gpytorch.kernels import MaternKernel\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.constraints import Interval\n",
    "#  acquisition_options\n",
    "# prior = {'eta':100}\n",
    "# acf_options\n",
    "# optimizer = axBOtorchOptimizer(params = params, agents = jv, models = ['SOBOL','BOTORCH_MODULAR','BOTORCH_MODULAR','BOTORCH_MODULAR'],n_batches = [1,30,20,30], batch_size = [10,2,2,2], metrics = metric, minimize_list = True, thresholds = None, ax_client = None,  max_parallelism = 10,model_kwargs_list = model_kwargs_list, model_gen_kwargs_list = model_gen_kwargs_list, name = 'ax_opti',parameter_constraints = parameter_constraints)\n",
    "# optimizer = axBOtorchOptimizer(params = params, agents = jv, models = ['SOBOL','BOTORCH_MODULAR','BOTORCH_MODULAR','BOTORCH_MODULAR'],n_batches = [1,30,20,30], batch_size = [100,2,2,2], metrics = metric, minimize_list = True, thresholds = None, ax_client = None,  max_parallelism = 10,model_kwargs_list = model_kwargs_list, model_gen_kwargs_list = model_gen_kwargs_list, name = 'ax_opti',parameter_constraints = parameter_constraints)\n",
    "# from ax.models.torch.botorch_modular.utils import ModelConfig\n",
    "# model_config = ModelConfig(covar_module_class='MaternKernel')\n",
    "# from gpytorch.constraints import GreaterThan\n",
    "# # (raw_lengthscale_constraint): Positive()\n",
    "from ax.models.torch.botorch_modular.utils import ModelConfig\n",
    "from ax.models.torch.botorch_modular.surrogate import SurrogateSpec\n",
    "# from \n",
    "# model_config = ModelConfig(covar_module_class=MaternKernel, covar_module_options={'lengthscale_constraint':Interval(1e-6,100)})\n",
    "# model_kwargs_list = [{},{'torch_device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),'torch_dtype': torch.double,'botorch_acqf_class':qLogNoisyExpectedImprovement,'transforms':[RemoveFixed, Log,UnitX, StandardizeY],'surrogate':Surrogate(SingleTaskGP),'model_configs':model_config}]#,covar_module_optionsMaternKernellengthscale_constraint':Interval(1e-6,100)})}]\n",
    "# model_configs = [None,ModelConfig(covar_module_class=MaternKernel, covar_module_options={'lengthscale_constraint':Interval(1e-6,100)},torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),torch_dtype = torch.double)]\n",
    "# model_kwargs_list = {}\n",
    "model_configs = [{},{}]\n",
    "model_kwargs_list = [{},{'torch_device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),'torch_dtype': torch.double,'botorch_acqf_class':qLogNoisyExpectedImprovement,'transforms':[RemoveFixed, Log,UnitX, StandardizeY],'surrogate_spec':SurrogateSpec(model_configs=[ModelConfig(botorch_model_class=SingleTaskGP,covar_module_class=MaternKernel,covar_module_options={'lengthscale_constraint':Interval(1e-6,100)})])}]\n",
    "# n_batches = [1,140], batch_size = [40,4]                     \n",
    "optimizer = axBOtorchOptimizer(params = params, agents = jv, models = ['SOBOL','BOTORCH_MODULAR'],n_batches = [1,20], batch_size = [40,4], ax_client = None,  max_parallelism = 100,\n",
    "                    model_configs = model_configs, model_kwargs_list = model_kwargs_list, model_gen_kwargs_list = model_gen_kwargs_list, name = 'ax_opti',parameter_constraints = parameter_constraints,scheduler_logging_level = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.optimize(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = optimizer.ax_client\n",
    "best_parameters = ax_client.get_best_parameters()[0]\n",
    "print(best_parameters)\n",
    "jv.params_w(best_parameters,jv.params)\n",
    "print(jv.get_SIMsalabim_clean_cmd(jv.params))\n",
    "jv.package_SIMsalabim_files(jv.params,'simss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the evolution of the optimization\n",
    "render(ax_client.get_contour_plot(param_x=\"l2.mu_n\", param_y=\"l2.mu_p\", metric_name=optimizer.all_metrics[0]))\n",
    "# render(ax_client.get_contour_plot(param_x=\"l2.N_t_bulk\", param_y=\"l2.C_np_bulk\", metric_name=metric))\n",
    "from ax.plot.slice import plot_slice\n",
    "model = ax_client.generation_strategy.model\n",
    "\n",
    "# render(plot_slice(model=model, param_name=\"l2.mu_n\", metric_name=optimizer.all_metrics[0]))\n",
    "# render(plot_slice(model=model, param_name=\"l2.mu_p\", metric_name=optimizer.all_metrics[0]))\n",
    "render(plot_slice(model=model, param_name=\"l2.N_t_bulk\", metric_name=optimizer.all_metrics[0]))\n",
    "# render(plot_slice(model=model, param_name=\"l2.N_ions\", metric_name=optimizer.all_metrics[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ax_client.experiment.fetch_data()\n",
    "\n",
    "plt.plot(np.minimum.accumulate(data.df[\"mean\"]), label=\"Best value seen so far\")\n",
    "\n",
    "# plt.yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create dic with keys same as ax_client.experiment.trials[0].arm.parameters\n",
    "# from ax.core.base_trial import TrialStatus as T\n",
    "# dumdic = {}\n",
    "# for key in ax_client.experiment.trials[0].arm.parameters.keys():\n",
    "#     dumdic[key] = []\n",
    "\n",
    "# # fill the dic with the values of the parameters\n",
    "# for i in range(len(ax_client.experiment.trials)):\n",
    "#     if ax_client.experiment.trials[i].status == T.COMPLETED:\n",
    "#         for key in ax_client.experiment.trials[i].arm.parameters.keys():\n",
    "#             dumdic[key].append(ax_client.experiment.trials[i].arm.parameters[key])\n",
    "\n",
    "\n",
    "# data = ax_client.experiment.fetch_data().df\n",
    "\n",
    "# target1 = data[data['metric_name'] == optimizer.all_metrics[0]]['mean']\n",
    "\n",
    "# dumdic[optimizer.all_metrics[0]] = list(target1)\n",
    "\n",
    "\n",
    "# dumdic['iteration'] = list(data[data['metric_name'] == optimizer.all_metrics[0]]['trial_index'])\n",
    "\n",
    "# df = pd.DataFrame(dumdic)\n",
    "\n",
    "\n",
    "# for par in params:\n",
    "#     if par.name in df.columns:\n",
    "#         if par.rescale:\n",
    "#             if par.value_type == 'int':\n",
    "#                 df[par.name] = df[par.name] * par.stepsize\n",
    "#             else:\n",
    "#                 df[par.name] = df[par.name] * par.fscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of all parameters that are not 'fixed'\n",
    "from optimpv.posterior.posterior import *\n",
    "params_orig_dict = {}\n",
    "for p in params_orig:\n",
    "    params_orig_dict[p.name] = p.value\n",
    "fig_dens, ax_dens = plot_density_exploration(params, optimizer.all_metrics[0], optimizer = optimizer, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun the simulation with the best parameters\n",
    "yfit = jv.run(parameters=ax_client.get_best_parameters()[0])\n",
    "# print(jv.run_Ax(parameters=results.get_best_result(metric=metric,mode='min',filter_nan_and_inf=True).config))\n",
    "plt.figure(figsize=(20,20))\n",
    "for Gfrac in Gfracs:\n",
    "    plt.plot(X[X[:,1]==Gfrac,0],y[X[:,1]==Gfrac],label='Gfrac = '+str(Gfrac))\n",
    "    plt.plot(X[X[:,1]==Gfrac,0],yfit[X[:,1]==Gfrac],label='Gfrac = '+str(Gfrac)+' fit',linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.modelbridge.cross_validation import cross_validate\n",
    "from ax.plot.contour import interact_contour\n",
    "from ax.plot.diagnostic import interact_cross_validation\n",
    "cv_results = cross_validate(model)\n",
    "render(interact_cross_validation(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "Nres = 10\n",
    "objective_name = optimizer.all_metrics[0]\n",
    "model = optimizer.ax_client.generation_strategy.model\n",
    "# set \n",
    "    \n",
    "fig, ax = devils_plot(params, Nres, objective_name, model, loss, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_1d_posteriors(params, Nres, objective_name, model, loss, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_1D_2D_posterior(params, 'l2.mu_n', 'l2.mu_p', 10, objective_name, model, loss, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimpv.general.general import inv_loss_function\n",
    "dumdic = {}\n",
    "# create a dic with the keys of the parameters\n",
    "if isinstance(ax_client.experiment.trials[0], BatchTrial):# check if trial is a BatchTrial\n",
    "    for key in ax_client.experiment.trials[0].arms[0].parameters.keys():\n",
    "        dumdic[key] = []\n",
    "    \n",
    "    # fill the dic with the values of the parameters\n",
    "    for i in range(len(ax_client.experiment.trials)):\n",
    "        if ax_client.experiment.trials[i].status == T.COMPLETED:\n",
    "            for arm in ax_client.experiment.trials[i].arms:\n",
    "                for key in arm.parameters.keys():\n",
    "                    dumdic[key].append(arm.parameters[key])\n",
    "            # for key in ax_client.experiment.trials[i].arms[0].parameters.keys():\n",
    "            #     dumdic[key].append(ax_client.experiment.trials[i].arms[0].parameters[key])\n",
    "else:\n",
    "    for key in ax_client.experiment.trials[0].arm.parameters.keys():\n",
    "        dumdic[key] = []\n",
    "\n",
    "    # fill the dic with the values of the parameters\n",
    "    for i in range(len(ax_client.experiment.trials)):\n",
    "        if ax_client.experiment.trials[i].status == T.COMPLETED:\n",
    "            for key in ax_client.experiment.trials[i].arm.parameters.keys():\n",
    "                dumdic[key].append(ax_client.experiment.trials[i].arm.parameters[key])\n",
    "\n",
    "\n",
    "data = ax_client.experiment.fetch_data().df\n",
    "\n",
    "target1 = data[data['metric_name'] == objective_name]['mean']\n",
    "\n",
    "dumdic[objective_name] = list(inv_loss_function(target1,loss))\n",
    "\n",
    "\n",
    "dumdic['iteration'] = list(data[data['metric_name'] == objective_name]['trial_index'])\n",
    "\n",
    "df = pd.DataFrame(dumdic)\n",
    "# make a second df with the 10 best results\n",
    "df_best = df.nsmallest(10, objective_name)\n",
    "# all objectivevalues below 3\n",
    "# df_best = df[df[objective_name] < 3]\n",
    "print(df_best)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "lognorm = matplotlib.colors.LogNorm(vmin=df[objective_name].min(), vmax=df[objective_name].max())\n",
    "\n",
    "# params_name = ['l2.N_t_bulk', 'l2.mu_p']\n",
    "# params_name = ['l2.N_t_bulk', 'l1.N_t_int']\n",
    "# params_name = ['l2.N_t_bulk', 'l2.N_ions']\n",
    "params_name = ['l2.mu_n', 'l2.mu_p']\n",
    "sc = plt.scatter(df[params_name[0]], df[params_name[1]], c=df[objective_name], norm=lognorm, cmap='viridis_r')\n",
    "plt.scatter(df_best[params_name[0]], df_best[params_name[1]], c='grey', marker='o', label='10 best results')\n",
    "# plot best result\n",
    "plt.scatter(best_parameters[params_name[0]], best_parameters[params_name[1]], c='blue', marker='x', label='Best result')\n",
    "# plot initial points\n",
    "plt.scatter(params_orig_dict[params_name[0]], params_orig_dict[params_name[1]], c='red', marker='x', label='Initial values')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.colorbar(sc, label='JV_JV_mse')\n",
    "plt.xlabel(params_name[0])\n",
    "plt.ylabel(params_name[1])\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Make and density plot of df_best 1d\n",
    "\n",
    "for i in range(len(params)):\n",
    "    plt.figure()\n",
    "    \n",
    "    # hist with log scale y bins\n",
    "    logbins = np.geomspace(params[i].bounds[0], params[i].bounds[1], 100)\n",
    "    plt.hist(df_best[params[i].name], bins=logbins, alpha=0.5, label='10 best results', color='b')\n",
    "    # kde plt\n",
    "    sns.kdeplot(df_best[params[i].name], log_scale=True, label='10 best results', color='b')\n",
    "\n",
    "    plt.vlines(best_parameters[params[i].name], 0, 5, colors='r', linestyles='dashed', label='Best result')\n",
    "    plt.vlines(params_orig_dict[params[i].name], 0, 5, colors='k', linestyles='dashed', label='Initial values')\n",
    "    # add median mean std and IQR to plot as vlines and shaded area\n",
    "    plt.vlines(df_best[params[i].name].median(), 0, 5, colors='g', linestyles='dashed', label='Median')\n",
    "    plt.vlines(df_best[params[i].name].mean(), 0, 5, colors='orange', linestyles='dashed', label='Mean')\n",
    "    # plt.vlines(df_best[params[i].name].mean() + df_best[params[i].name].std(), 0, 3, colors='purple', linestyles='dashed', label='Mean + std')\n",
    "    # plt.vlines(df_best[params[i].name].mean() - df_best[params[i].name].std(), 0, 3, colors='purple', linestyles='dashed', label='Mean - std')\n",
    "    plt.fill_betweenx([0, 5], df_best[params[i].name].mean() - df_best[params[i].name].std(), df_best[params[i].name].mean() + df_best[params[i].name].std(), color='purple', alpha=0.3, label='Mean +/- std')\n",
    "    # plt.fill_betweenx([0, 5], df_best[params[i].name].quantile(0.25), df_best[params[i].name].quantile(0.75), color='g', alpha=0.3, label='IQR')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(params[i].bounds[0], params[i].bounds[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the output files (comment out if you want to keep the output files)\n",
    "sim.clean_all_output(session_path)\n",
    "sim.delete_folders('tmp',session_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
