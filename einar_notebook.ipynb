{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings,os,uuid\n",
    "# remove warnings from ax\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import ax, torch\n",
    "from optimpv import *\n",
    "from optimpv.axBOtorch.axUtils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, copy, os\n",
    "from itertools import combinations\n",
    "from ax import *\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "from botorch.models import SaasFullyBayesianSingleTaskGP, SingleTaskGP\n",
    "\n",
    "init_notebook_plotting()\n",
    "\n",
    "# define the parameters\n",
    "params = []\n",
    "\n",
    "mun = FitParam(name = 'l1.mu_n', value = 2e-8, bounds = [1e-8,1e-7], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = 1e-8, rescale = True, stepsize = None, display_name=r'$\\mu_n$', unit='m$^2$ V$^{-1}$s$^{-1}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(mun)\n",
    "\n",
    "mup = FitParam(name = 'l1.mu_p', value = 8e-8, bounds = [1e-8,1e-7], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = 1e-8, rescale = True, stepsize = None, display_name=r'$\\mu_p$', unit='m$^2$ V$^{-1}$s$^{-1}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(mup)\n",
    "\n",
    "k_direct = FitParam(name = 'l1.k_direct', value = 1e-17, bounds = [1e-18,1e-16], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = True, stepsize = None, display_name=r'$k_{direct}$', unit='m$^3$ s$^{-1}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(k_direct)\n",
    "\n",
    "Nc = FitParam(name = 'l1.N_c', value = 1e27, bounds = [5e26,5e27], values = None, start_value = None, log_scale = True, value_type = 'float', fscale = None, rescale = False, stepsize = None, display_name=r'$N_c$', unit='m$^{-3}$', axis_type = 'log', std = 0,encoding = None,force_log = False)\n",
    "params.append(Nc)\n",
    "\n",
    "\n",
    "# original values\n",
    "params_orig = copy.deepcopy(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pySIMsalabim as sim\n",
    "from pySIMsalabim.experiments.JV_steady_state import *\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "session_path = os.path.join(curr_dir, 'SIMsalabim','SimSS')\n",
    "simss_device_parameters = os.path.join(session_path, 'simulation_setup.txt')\n",
    "\n",
    "# Set the JV parameters\n",
    "Gfracs = [0.1,0.5,1.0] # Fractions of the generation rate to simulate\n",
    "# Gfracs = None\n",
    "UUID = str(uuid.uuid4())\n",
    "\n",
    "cmd_pars = []\n",
    "for param in params:\n",
    "    cmd_pars.append({'par':param.name, 'val':str(param.value)})\n",
    "\n",
    "\n",
    "# Run the JV simulation\n",
    "ret, mess = run_SS_JV(simss_device_parameters, session_path, JV_file_name = 'JV.dat', varFile= 'Var.dat',G_fracs = Gfracs, parallel = False, max_jobs = 3, UUID=UUID, cmd_pars=cmd_pars)\n",
    "\n",
    "# import random noise\n",
    "from numpy.random import default_rng\n",
    "# save data for fitting\n",
    "X,y = [],[]\n",
    "if Gfracs is None:\n",
    "    data = pd.read_csv(os.path.join(session_path, 'JV_'+UUID+'.dat'), sep=r'\\s+') # Load the data\n",
    "    Vext = np.asarray(data['Vext'].values)\n",
    "    Jext = np.asarray(data['Jext'].values)\n",
    "    G = np.ones_like(Vext)\n",
    "    rng = default_rng()#\n",
    "    noise = rng.standard_normal(Jext.shape) * 0.01 * Jext\n",
    "    Jext = Jext + noise\n",
    "    X= Vext\n",
    "    y = Jext\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(X,y)\n",
    "    plt.show()\n",
    "else:\n",
    "    for Gfrac in Gfracs:\n",
    "        data = pd.read_csv(os.path.join(session_path, 'JV_Gfrac_'+str(Gfrac)+'_'+UUID+'.dat'), sep=r'\\s+') # Load the data\n",
    "        Vext = np.asarray(data['Vext'].values)\n",
    "        Jext = np.asarray(data['Jext'].values)\n",
    "        G = np.ones_like(Vext)*Gfrac\n",
    "        rng = default_rng()#\n",
    "        noise = rng.standard_normal(Jext.shape) * 0.005 * Jext\n",
    "        Jext = Jext + noise\n",
    "\n",
    "        if len(X) == 0:\n",
    "            X = np.vstack((Vext,G)).T\n",
    "            y = Jext\n",
    "        else:\n",
    "            X = np.vstack((X,np.vstack((Vext,G)).T))\n",
    "            y = np.hstack((y,Jext))\n",
    "\n",
    "    # remove all the current where Jext is positive i.e. above Voc\n",
    "    X = X[y<100]\n",
    "    y = y[y<100]\n",
    "\n",
    "    plt.figure()\n",
    "    for Gfrac in Gfracs:\n",
    "        plt.plot(X[X[:,1]==Gfrac,0],y[X[:,1]==Gfrac],label='Gfrac = '+str(Gfrac))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimpv.DDfits.JVAgent import JVAgent\n",
    "metric = 'mse'\n",
    "loss = 'soft_l1'\n",
    "\n",
    "jv = JVAgent(params, X, y, session_path, simss_device_parameters, parallel = False, max_jobs = 3, metric = metric, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimpv.axBOtorch.axBOtorchOptimizer import axBOtorchOptimizer\n",
    "from botorch.acquisition.logei import qLogNoisyExpectedImprovement\n",
    "from ax.modelbridge.transforms.standardize_y import StandardizeY\n",
    "from ax.modelbridge.transforms.unit_x import UnitX\n",
    "from ax.modelbridge.transforms.remove_fixed import RemoveFixed\n",
    "from ax.modelbridge.transforms.log import Log\n",
    "\n",
    "model_gen_kwargs_list = None\n",
    "parameter_constraints = None\n",
    "\n",
    "model_kwargs_list = [{},{'torch_device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),'torch_dtype': torch.double,'botorch_acqf_class':qLogNoisyExpectedImprovement,'transforms':[RemoveFixed, Log,UnitX, StandardizeY]}]\n",
    "# n_batches = [1,140], batch_size = [40,4]                     \n",
    "optimizer = axBOtorchOptimizer(params = params, agents = jv, models = ['SOBOL','BOTORCH_MODULAR'],n_batches = [10,30], batch_size = [4,2], ax_client = None,  max_parallelism = 100,\n",
    "                   model_kwargs_list = model_kwargs_list, model_gen_kwargs_list = model_gen_kwargs_list, name = 'ax_opti',parameter_constraints = parameter_constraints,scheduler_logging_level = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.optimize(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = optimizer.ax_client\n",
    "best_parameters = ax_client.get_best_parameters()[0]\n",
    "print(best_parameters)\n",
    "jv.params_w(best_parameters,jv.params)\n",
    "print(jv.params)\n",
    "print(jv.get_SIMsalabim_clean_cmd(jv.params))\n",
    "jv.package_SIMsalabim_files(jv.params,'simss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the evolution of the optimization\n",
    "render(ax_client.get_contour_plot(param_x=\"l1.mu_n\", param_y=\"l1.mu_p\", metric_name=optimizer.all_metrics[0]))\n",
    "\n",
    "from ax.plot.slice import plot_slice\n",
    "model = ax_client.generation_strategy.model\n",
    "\n",
    "render(plot_slice(model=model, param_name=\"l1.k_direct\", metric_name=optimizer.all_metrics[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ax_client.experiment.fetch_data()\n",
    "\n",
    "plt.plot(np.minimum.accumulate(data.df[\"mean\"]), label=\"Best value seen so far\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of all parameters that are not 'fixed'\n",
    "from optimpv.posterior.posterior import *\n",
    "\n",
    "params_orig_dict = {}\n",
    "for idx, p in enumerate(params_orig):\n",
    "    if p.value_type == 'float':\n",
    "        if p.force_log:\n",
    "            best_parameters[p.name] = 10**(best_parameters[p.name])\n",
    "        else:\n",
    "            best_parameters[p.name] = best_parameters[p.name]*p.fscale\n",
    "    elif p.value_type == 'int':\n",
    "        best_parameters[p.name] = best_parameters[p.name]*p.stepsize\n",
    "    else:\n",
    "        best_parameters[p.name] = best_parameters[p.name]\n",
    "\n",
    "    params_orig_dict[p.name] = p.value\n",
    "\n",
    "\n",
    "fig_dens, ax_dens = plot_density_exploration(params, optimizer.all_metrics[0], optimizer = optimizer, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun the simulation with the best parameters\n",
    "yfit = jv.run(parameters=ax_client.get_best_parameters()[0])\n",
    "# print(jv.run_Ax(parameters=results.get_best_result(metric=metric,mode='min',filter_nan_and_inf=True).config))\n",
    "plt.figure(figsize=(10,10))\n",
    "for Gfrac in Gfracs:\n",
    "    plt.plot(X[X[:,1]==Gfrac,0],y[X[:,1]==Gfrac],label='Gfrac = '+str(Gfrac))\n",
    "    plt.plot(X[X[:,1]==Gfrac,0],yfit[X[:,1]==Gfrac],label='Gfrac = '+str(Gfrac)+' fit',linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.modelbridge.cross_validation import cross_validate\n",
    "from ax.plot.contour import interact_contour\n",
    "from ax.plot.diagnostic import interact_cross_validation\n",
    "cv_results = cross_validate(model)\n",
    "render(interact_cross_validation(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush cuda memory\n",
    "Nres = 10\n",
    "objective_name = optimizer.all_metrics[0]\n",
    "model = optimizer.ax_client.generation_strategy.model\n",
    "# set \n",
    "    \n",
    "fig, ax = devils_plot(params, Nres, objective_name, model, loss, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_1d_posteriors(params, Nres, objective_name, model, loss, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_1D_2D_posterior(params, 'l1.mu_n', 'l1.mu_p', 10, objective_name, model, loss, best_parameters = best_parameters, params_orig = params_orig_dict, optimizer_type = 'ax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimpv.general.general import inv_loss_function\n",
    "df = get_df_from_ax(params, optimizer.all_metrics[0], optimizer = optimizer)\n",
    "\n",
    "# make a second df with the 10 best results\n",
    "df_best = df.nsmallest(10, objective_name)\n",
    "# all objectivevalues below 3\n",
    "# df_best = df[df[objective_name] < 3]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "lognorm = matplotlib.colors.LogNorm(vmin=df[objective_name].min(), vmax=df[objective_name].max())\n",
    "\n",
    "# params_name = ['l2.N_t_bulk', 'l2.mu_p']\n",
    "# params_name = ['l2.N_t_bulk', 'l1.N_t_int']\n",
    "# params_name = ['l2.N_t_bulk', 'l2.N_ions']\n",
    "params_name = ['l1.mu_n', 'l1.mu_p']\n",
    "sc = plt.scatter(df[params_name[0]], df[params_name[1]], c=df[objective_name], norm=lognorm, cmap='viridis_r')\n",
    "plt.scatter(df_best[params_name[0]], df_best[params_name[1]], c='grey', marker='o', label='10 best results')\n",
    "# plot best result\n",
    "plt.scatter(best_parameters[params_name[0]], best_parameters[params_name[1]], c='blue', marker='x', label='Best result')\n",
    "# plot initial points\n",
    "plt.scatter(params_orig_dict[params_name[0]], params_orig_dict[params_name[1]], c='red', marker='x', label='Initial values')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.colorbar(sc, label='JV_JV_mse')\n",
    "plt.xlabel(params_name[0])\n",
    "plt.ylabel(params_name[1])\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Make and density plot of df_best 1d\n",
    "\n",
    "for i in range(len(params)):\n",
    "    plt.figure()\n",
    "    \n",
    "    # hist with log scale y bins\n",
    "    logbins = np.geomspace(params[i].bounds[0], params[i].bounds[1], 100)\n",
    "    plt.hist(df_best[params[i].name], bins=logbins, alpha=0.5, label='10 best results', color='b')\n",
    "    # kde plt\n",
    "    sns.kdeplot(df_best[params[i].name], log_scale=True, label='10 best results', color='b')\n",
    "\n",
    "    plt.vlines(best_parameters[params[i].name], 0, 5, colors='r', linestyles='dashed', label='Best result')\n",
    "    plt.vlines(params_orig_dict[params[i].name], 0, 5, colors='k', linestyles='dashed', label='Initial values')\n",
    "    # add median mean std and IQR to plot as vlines and shaded area\n",
    "    plt.vlines(df_best[params[i].name].median(), 0, 5, colors='g', linestyles='dashed', label='Median')\n",
    "    plt.vlines(df_best[params[i].name].mean(), 0, 5, colors='orange', linestyles='dashed', label='Mean')\n",
    "    # plt.vlines(df_best[params[i].name].mean() + df_best[params[i].name].std(), 0, 3, colors='purple', linestyles='dashed', label='Mean + std')\n",
    "    # plt.vlines(df_best[params[i].name].mean() - df_best[params[i].name].std(), 0, 3, colors='purple', linestyles='dashed', label='Mean - std')\n",
    "    plt.fill_betweenx([0, 5], df_best[params[i].name].mean() - df_best[params[i].name].std(), df_best[params[i].name].mean() + df_best[params[i].name].std(), color='purple', alpha=0.3, label='Mean +/- std')\n",
    "    # plt.fill_betweenx([0, 5], df_best[params[i].name].quantile(0.25), df_best[params[i].name].quantile(0.75), color='g', alpha=0.3, label='IQR')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(params[i].bounds[0], params[i].bounds[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the output files (comment out if you want to keep the output files)\n",
    "sim.clean_all_output(session_path)\n",
    "sim.delete_folders('tmp',session_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
